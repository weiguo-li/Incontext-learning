{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3Zsa50vycAoa",
      "metadata": {
        "id": "3Zsa50vycAoa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# sys.path.append(\"./Incontext-learning\") # this part works for goole colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "CM2tO10Z552-",
      "metadata": {
        "id": "CM2tO10Z552-"
      },
      "outputs": [],
      "source": [
        "import utility\n",
        "import metric\n",
        "import importlib\n",
        "importlib.reload(utility)\n",
        "importlib.reload(metric)\n",
        "from utility import (data_selection, evaluate_zeroshot,evaluate_finetuning,evaluate_demonstrations as  evaluate_fewshots, finetune_model_eval)\n",
        "from metric import Rec2FTP, SimAOU\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "937daa06",
      "metadata": {
        "id": "937daa06"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Yev1fLLLKSqF",
      "metadata": {
        "id": "Yev1fLLLKSqF"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d88f461b",
      "metadata": {
        "id": "d88f461b"
      },
      "source": [
        "# check point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ddf39116",
      "metadata": {
        "id": "ddf39116"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load SST-2 dataset\n",
        "# dataset = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "# load SST-5 dataset\n",
        "dataset = load_dataset(\"SetFit/sst5\")\n",
        "\n",
        "# Load Qwen3 tokenizer and model\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "model_path = \"/home/students/wli/UniHeidelberg/semster2/final_projects/models/Qwen3-0.6B-Base\"\n",
        "# model_path = model_name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map = \"auto\")\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "# Make sure tokenizer has pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# if getattr(model.config.pad_token_id, None) is None:\n",
        "    # pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d659a25c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 8544\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 1101\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 2210\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "VGnN8Sili5wE",
      "metadata": {
        "id": "VGnN8Sili5wE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Sep 20 02:02:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1080 Ti     Off |   00000000:3B:00.0 Off |                  N/A |\n",
            "| 25%   39C    P5             15W /  250W |    1295MiB /  11264MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  NVIDIA GeForce GTX 1080 Ti     Off |   00000000:5E:00.0 Off |                  N/A |\n",
            "| 25%   33C    P2             55W /  250W |    1283MiB /  11264MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A    168965      C   ...al-form-attention/.venv/bin/python3       1290MiB |\n",
            "|    1   N/A  N/A    168965      C   ...al-form-attention/.venv/bin/python3       1278MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a3a6126b",
      "metadata": {
        "id": "a3a6126b"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"] # the orginal paper use this\n",
        "test_dataset = dataset[\"validation\"]\n",
        "\n",
        "def add_idx(example, idx):\n",
        "    example[\"idx\"] = idx\n",
        "    return example\n",
        "\n",
        "test_dataset = test_dataset.map(add_idx, with_indices=True)\n",
        "# Format examples as causal LM inputs\n",
        "def preprocess_function(examples):\n",
        "    label_map = {0: \"negative\", 1: \"positive\"}\n",
        "    inputs = [\n",
        "        f\"Sentence: {sentence} Label: {label_map[label]}\"\n",
        "        for sentence, label in zip(examples[\"sentence\"], examples[\"label\"])\n",
        "    ]\n",
        "    # Tokenize with padding/truncation\n",
        "    tokenized = tokenizer(\n",
        "        inputs,\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=256,\n",
        "        # return_tensors=\"pt\"\n",
        "        return_tensors=None\n",
        "    )\n",
        "    # Set labels equal to input_ids for causal LM loss\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "def preprocess_function_sst5(examples):\n",
        "    label_map = {0: \"terrible\", 1: \"bad\", 2: \"neutral\", 3: \"good\", 4: \"very great\"}\n",
        "    inputs = [\n",
        "        f\"Sentence: {sentence} Label: {label_map[label]}\"\n",
        "        for sentence, label in zip(examples[\"text\"], examples[\"label\"])\n",
        "    ]\n",
        "    # Tokenize with padding/truncation\n",
        "    tokenized = tokenizer(\n",
        "        inputs,\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=256,\n",
        "        # return_tensors=\"pt\"\n",
        "        return_tensors=None\n",
        "    )\n",
        "    # Set labels equal to input_ids for causal LM loss\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# Tokenize the dataset\n",
        "# tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "train_tokenized_datasets = train_dataset.map(preprocess_function_sst5, batched = True, remove_columns = train_dataset.column_names)\n",
        "test_tokenized_datasets = test_dataset.map(preprocess_function_sst5, batched=True, remove_columns=test_dataset.column_names)\n",
        "# Data collator (handles padding dynamically in batch)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "VfOgObQEcX31",
      "metadata": {
        "id": "VfOgObQEcX31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films. Label: \"\n"
          ]
        }
      ],
      "source": [
        "# test the behavior of LLM before fine tuing\n",
        "prompt = \"Sentence: a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films. Label:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "# You can control max_length or early stopping\n",
        "output_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1,    # adjust based on expected label length\n",
        "    do_sample=False,       # greedy decoding\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# Decode generated tokens\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9f0f4688",
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_dataset = test_dataset.select(range(32)) # use a smaller test set for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b78a38d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': [\"although there are several truly jolting scares , there 's also an abundance of hackneyed dialogue and more silly satanic business than you can shake a severed limb at .\", \"it believes it 's revealing some great human truths , when , in reality , it 's churning ground that has long passed the point of being fertile .\"], 'label': tensor([2, 2]), 'label_text': ['neutral', 'neutral'], 'idx': tensor([836, 911])}\n",
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "for batch in DataLoader(test_dataset, batch_size=2, shuffle=True):\n",
        "    print(batch)\n",
        "    print(type(batch))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f651e2a7",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce1a8d3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/138 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 2303.54 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/138 [00:09<21:39,  9.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 2333.88 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 2/138 [00:18<20:37,  9.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 3199.49 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 3/138 [00:27<20:38,  9.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 3199.54 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 4/138 [00:36<20:22,  9.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 4070.29 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▎         | 5/138 [00:45<20:24,  9.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 4915.75 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 6/138 [00:55<20:20,  9.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 4915.76 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 7/138 [01:04<20:07,  9.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.20 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 8/138 [01:13<20:03,  9.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.21 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 9/138 [01:22<19:38,  9.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.30 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 10/138 [01:31<19:27,  9.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.43 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 11/138 [01:40<19:09,  9.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.43 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 12/138 [01:49<18:49,  8.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.43 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 13/138 [01:58<18:42,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.43 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 14/138 [02:07<18:24,  8.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.43 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 15/138 [02:15<18:09,  8.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.44 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 16/138 [02:25<18:12,  8.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5791.44 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 17/138 [02:33<17:56,  8.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 5821.29 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 18/138 [02:42<17:55,  8.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 6667.59 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 19/138 [02:52<18:00,  9.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7516.15 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 20/138 [03:01<18:01,  9.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7516.46 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 21/138 [03:10<17:47,  9.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7516.47 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 22/138 [03:19<17:35,  9.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7516.48 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 23/138 [03:28<17:30,  9.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.08 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 24/138 [03:38<17:25,  9.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.08 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 25/138 [03:47<17:05,  9.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.08 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 26/138 [03:55<16:46,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.08 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 27/138 [04:04<16:30,  8.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.08 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 28/138 [04:13<16:18,  8.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.09 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 29/138 [04:22<16:07,  8.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.09 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 30/138 [04:31<16:04,  8.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.10 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 31/138 [04:40<15:59,  8.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.10 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 32/138 [04:49<15:55,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.10 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 33/138 [04:58<15:39,  8.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 7577.10 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▍       | 34/138 [05:07<15:24,  8.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 8029.94 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 35/138 [05:16<15:21,  8.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 8030.00 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 36/138 [05:25<15:12,  8.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage: 8030.02 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 37/138 [05:33<15:01,  8.92s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "a = SimAOU(model,tokenizer,train_dataset, test_dataset, best_seed = 0, num_data_points = 32, data_type = \"sst5\", batch_size_icl = 32, batch_size_zsl = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef8f7ccb",
      "metadata": {},
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a228d4dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# finetune_model_eval(model, tokenizer,train_dataset,test_dataset, best_seed = 50, data_type=\"sst5\", num_data_points = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1053b02",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NanoGPT",
      "language": "python",
      "name": "nanogpt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
