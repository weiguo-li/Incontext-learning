{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Incontext-learning\") # this part works for goole colab"
      ],
      "metadata": {
        "id": "3Zsa50vycAoa"
      },
      "id": "3Zsa50vycAoa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "937daa06",
      "metadata": {
        "id": "937daa06"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Yev1fLLLKSqF"
      },
      "id": "Yev1fLLLKSqF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f71c6230"
      },
      "source": [
        "# Task\n",
        "Modify the provided code to perform a classification task instead of a causal language modeling task. This involves loading a suitable classification model, preparing a classification dataset, fine-tuning the model on the dataset, and evaluating its performance."
      ],
      "id": "f71c6230"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dc44411"
      },
      "source": [
        "## Load a classification model\n",
        "\n",
        "### Subtask:\n",
        "Instead of loading a causal language model, load a model specifically designed for sequence classification, such as `AutoModelForSequenceClassification`.\n"
      ],
      "id": "2dc44411"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5bcafc8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to load a sequence classification model. This requires importing the appropriate class and loading the model and tokenizer.\n",
        "\n"
      ],
      "id": "d5bcafc8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2f22bea"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-0.6B\" # Assuming a sequence classification version exists\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the padding token to the EOS token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Load the sequence classification model\n",
        "# Note: You might need to specify `num_labels` based on your classification task\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels = 2).to(device)\n",
        "\n",
        "print(\"Sequence classification model and tokenizer loaded successfully.\")"
      ],
      "id": "f2f22bea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check current tokens\n",
        "print(\"EOS token:\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
        "print(\"PAD token:\", tokenizer.pad_token, tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "YNWo2sTFE1kQ"
      },
      "id": "YNWo2sTFE1kQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ab247f"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess your classification dataset. This includes tokenizing the text and formatting the labels.\n"
      ],
      "id": "42ab247f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "777a0a34"
      },
      "source": [
        "**Reasoning**:\n",
        "Load a suitable classification dataset, define a preprocessing function to tokenize the text and prepare labels, and apply the function to the dataset.\n",
        "\n"
      ],
      "id": "777a0a34"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05fc89f4"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a suitable classification dataset (e.g., SST-2 from GLUE)\n",
        "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "# Define a preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the text\n",
        "    tokenized_inputs = tokenizer(examples[\"sentence\"], truncation=True)\n",
        "    # Prepare the labels\n",
        "    tokenized_inputs[\"labels\"] = examples[\"label\"]\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Split into training and evaluation sets\n",
        "# train_dataset = tokenized_dataset[\"validation\"]\n",
        "# eval_dataset = tokenized_dataset[\"validation\"]\n",
        "train_dataset = tokenized_dataset.select(range(100))\n",
        "eval_dataset = tokenized_dataset.select(range(100, 200))\n",
        "\n",
        "print(\"Dataset loaded, preprocessed, and split successfully.\")\n",
        "print(\"Training dataset example:\", train_dataset[0])\n",
        "print(\"Evaluation dataset example:\", eval_dataset[0])"
      ],
      "id": "05fc89f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "id": "4GGIYOK-8Ayz"
      },
      "id": "4GGIYOK-8Ayz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cb8ce98"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "### Subtask:\n",
        "Train the classification model on your prepared dataset.\n"
      ],
      "id": "5cb8ce98"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7d23bb"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classes and define the training arguments, then initialize and start the training process.\n",
        "\n"
      ],
      "id": "ed7d23bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad32eb23"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `evaluation_strategy` is not a valid argument for `TrainingArguments`. Looking at the transformers documentation, `evaluation_strategy` was deprecated and replaced by `eval_strategy`. I will fix the typo and rerun the code.\n",
        "\n"
      ],
      "id": "ad32eb23"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jirgemdPzgSz"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Output directory\n",
        "    eval_strategy=\"epoch\",  # Evaluate every epoch\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=1,  # Number of training epochs.  || in order to make it the same as in context learning\n",
        "    weight_decay=0.01,  # Weight decay\n",
        "    logging_dir=\"./logs\",  # Directory for logs\n",
        "    logging_steps=10, # Log every 10 steps\n",
        "    report_to=\"none\", # Disable Weights & Biases\n",
        "    save_strategy='no'\n",
        ")\n",
        "\n",
        "# Ensure the tokenizer has the correct padding token and ID for the data collator\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "if model.config.pad_token_id is None:\n",
        "  model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "# Initialize the data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # the loaded sequence classification model\n",
        "    args=training_args,  # training arguments\n",
        "    train_dataset=train_dataset,  # training dataset\n",
        "    eval_dataset=eval_dataset,  # evaluation dataset\n",
        "    data_collator=data_collator, # Use the data collator\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "id": "jirgemdPzgSz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NanoGPT",
      "language": "python",
      "name": "nanogpt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}